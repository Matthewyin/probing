---
type: "always_apply"
---

 AI Agent åäºŒå› å­åº”ç”¨ç¼–ç å‡†åˆ™

 ç¬¬ä¸€ç« ï¼šä»£ç åº“ä¸ç‰ˆæœ¬æ§åˆ¶ (Codebase)

 è§„åˆ™ 1.1ï¼šã€å¼ºåˆ¶æ‰§è¡Œã€‘å•ä¸€ä»£ç åº“åŸåˆ™

åšä»€ä¹ˆ (What)ï¼š
- ä¸€ä¸ªåº”ç”¨å¯¹åº”ä¸€ä¸ªä»£ç åº“ï¼Œå¤šä¸ªç¯å¢ƒå…±äº«åŒä¸€ä»£ç åº“
- ç»å¯¹ç¦æ­¢ä¸ºä¸åŒç¯å¢ƒç»´æŠ¤ä¸åŒçš„ä»£ç åˆ†æ”¯
- æ‰€æœ‰ç¯å¢ƒå·®å¼‚é€šè¿‡é…ç½®ç®¡ç†è§£å†³

æ€ä¹ˆåš (How)ï¼š
bash
 æ­£ç¡®çš„é¡¹ç›®ç»“æ„
/my-agent
â”œâ”€â”€ .env.development      å¼€å‘ç¯å¢ƒé…ç½®æ¨¡æ¿
â”œâ”€â”€ .env.staging         æµ‹è¯•ç¯å¢ƒé…ç½®æ¨¡æ¿  
â”œâ”€â”€ .env.production      ç”Ÿäº§ç¯å¢ƒé…ç½®æ¨¡æ¿
â”œâ”€â”€ docker-compose.yml   æœ¬åœ°å¼€å‘ç¯å¢ƒ
â”œâ”€â”€ Dockerfile          ç»Ÿä¸€çš„æ„å»ºé…ç½®
â””â”€â”€ src/               ç»Ÿä¸€çš„ä»£ç åº“


 ç¬¬äºŒç« ï¼šä¾èµ–ç®¡ç† (Dependencies)

 è§„åˆ™ 2.1ï¼šã€ç»å¯¹ç¦æ­¢ã€‘éšå¼ä¾èµ–

åšä»€ä¹ˆ (What)ï¼š
- æ˜¾å¼å£°æ˜æ‰€æœ‰ä¾èµ–é¡¹åŠå…¶ç²¾ç¡®ç‰ˆæœ¬
- ä½¿ç”¨ä¾èµ–éš”ç¦»å·¥å…·ç¡®ä¿ç¯å¢ƒä¸€è‡´æ€§
- ç»ä¸ä¾èµ–ç³»ç»Ÿçº§çš„éšå¼åŒ…æˆ–å·¥å…·

æ€ä¹ˆåš (How)ï¼š

Python ç¤ºä¾‹ï¼š
python
 requirements.txt - é”å®šç²¾ç¡®ç‰ˆæœ¬
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-dotenv==1.0.0

 é¦–é€‰ä½¿ç”¨uvè™šæ‹Ÿç¯å¢ƒéš”ç¦»ä¾èµ–
uv
source venv/bin/activate   Linux/Mac
pip install -r requirements.txt



Node.js ç¤ºä¾‹ï¼š
json
// package.json - ä½¿ç”¨ç²¾ç¡®ç‰ˆæœ¬æˆ–èŒƒå›´
{
  "dependencies": {
    "express": "4.18.2",
    "zod": "^3.22.4",
    "dotenv": "16.3.1"
  },
  "engines": {
    "node": ">=18.0.0",
    "npm": ">=9.0.0"
  }
}


 ç¬¬ä¸‰ç« ï¼šé…ç½®ç®¡ç† (Config)

 è§„åˆ™ 3.1ï¼šã€ç»å¯¹ç¦æ­¢ã€‘ç¡¬ç¼–ç é…ç½®

åšä»€ä¹ˆ (What)ï¼š
- æ‰€æœ‰é…ç½®ä¿¡æ¯å¿…é¡»å¤–éƒ¨åŒ–åˆ°ç¯å¢ƒå˜é‡
- ä¸¥æ ¼åŒºåˆ†é…ç½®å’Œå¸¸é‡
- æ”¯æŒé…ç½®çš„åˆ†å±‚åŠ è½½å’ŒéªŒè¯

æ€ä¹ˆåš (How)ï¼š

Python + Pydantic ç¤ºä¾‹ï¼š
python
 config.py
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Optional

class AppSettings(BaseSettings):
     åŸºç¡€é…ç½®
    APP_NAME: str = "AI Agent"
    APP_VERSION: str = "1.0.0"
    DEBUG: bool = False
    
     æ•°æ®åº“é…ç½®
    DATABASE_URL: str
    DATABASE_POOL_SIZE: int = 5
    
     AIæœåŠ¡é…ç½®
    OPENAI_API_KEY: str
    OPENAI_MODEL: str = "gpt-4"
    TEMPERATURE: float = 0.7
    
     Redisé…ç½®
    REDIS_URL: str
    REDIS_TTL: int = 3600
    
     å®‰å…¨é…ç½®
    SECRET_KEY: str
    JWT_EXPIRE_MINUTES: int = 30
    
    model_config = SettingsConfigDict(
        env_file='.env',
        env_file_encoding='utf-8',
        case_sensitive=True,
        extra='ignore'
    )
    
    def __init__(self, kwargs):
        super().__init__(kwargs)
         å¯åŠ¨æ—¶éªŒè¯å…³é”®é…ç½®
        self._validate_critical_configs()
    
    def _validate_critical_configs(self):
        if not self.DATABASE_URL:
            raise ValueError("DATABASE_URL is required")
        if not self.SECRET_KEY or len(self.SECRET_KEY) < 32:
            raise ValueError("SECRET_KEY must be at least 32 characters")

 å…¨å±€é…ç½®å®ä¾‹
settings = AppSettings()


ç¯å¢ƒå˜é‡æ–‡ä»¶ç¤ºä¾‹ï¼š
bash
 .env.production
APP_NAME="Production AI Agent"
DEBUG=false
DATABASE_URL="postgresql://user:pass@prod-db:5432/agent_db"
REDIS_URL="redis://prod-redis:6379/0"
OPENAI_API_KEY="sk-prod-key-xxxxxxxx"
SECRET_KEY="super-secure-production-secret-key-32-chars-min"




 ç¬¬å››ç« ï¼šåç«¯æœåŠ¡ (Backing Services)

 è§„åˆ™ 4.1ï¼šã€å¼ºåˆ¶æ‰§è¡Œã€‘æœåŠ¡æŠ½è±¡åŒ–

åšä»€ä¹ˆ (What)ï¼š
- å°†æ‰€æœ‰åç«¯æœåŠ¡ï¼ˆæ•°æ®åº“ã€ç¼“å­˜ã€æ¶ˆæ¯é˜Ÿåˆ—ï¼‰è§†ä¸ºé™„åŠ èµ„æº
- é€šè¿‡ç»Ÿä¸€çš„æ¥å£è®¿é—®æœåŠ¡ï¼Œæ”¯æŒæœåŠ¡çš„çƒ­åˆ‡æ¢
- æœåŠ¡è¿æ¥ä¿¡æ¯å®Œå…¨é€šè¿‡é…ç½®ç®¡ç†

æ€ä¹ˆåš (How)ï¼š

æœåŠ¡æŠ½è±¡å±‚ç¤ºä¾‹ï¼š
python
 services/interfaces.py
from abc import ABC, abstractmethod
from typing import Any, Optional

class CacheService(ABC):
    @abstractmethod
    async def get(self, key: str) -> Optional[str]:
        pass
    
    @abstractmethod
    async def set(self, key: str, value: str, ttl: int = None) -> bool:
        pass

class DatabaseService(ABC):
    @abstractmethod
    async def execute_query(self, query: str, params: dict = None) -> Any:
        pass

 services/implementations.py
import redis.asyncio as redis
import asyncpg
from services.interfaces import CacheService, DatabaseService

class RedisCache(CacheService):
    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url)
    
    async def get(self, key: str) -> Optional[str]:
        return await self.redis.get(key)
    
    async def set(self, key: str, value: str, ttl: int = None) -> bool:
        return await self.redis.set(key, value, ex=ttl)

class PostgresDatabase(DatabaseService):
    def __init__(self, database_url: str):
        self.pool = None
        self.database_url = database_url
    
    async def execute_query(self, query: str, params: dict = None):
        if not self.pool:
            self.pool = await asyncpg.create_pool(self.database_url)
        async with self.pool.acquire() as conn:
            return await conn.fetch(query, (params or {}))

 ä¾èµ–æ³¨å…¥å®¹å™¨
class ServiceContainer:
    def __init__(self, settings: AppSettings):
        self.cache: CacheService = RedisCache(settings.REDIS_URL)
        self.database: DatabaseService = PostgresDatabase(settings.DATABASE_URL)


 ç¬¬äº”ç« ï¼šæ„å»ºã€å‘å¸ƒã€è¿è¡Œ (Build, Release, Run)

 è§„åˆ™ 5.1ï¼šã€ä¸¥æ ¼åˆ†ç¦»ã€‘ä¸‰é˜¶æ®µéƒ¨ç½²

åšä»€ä¹ˆ (What)ï¼š
- æ„å»ºé˜¶æ®µï¼šå°†ä»£ç è½¬æ¢ä¸ºå¯æ‰§è¡ŒåŒ…
- å‘å¸ƒé˜¶æ®µï¼šå°†æ„å»ºäº§ç‰©ä¸é…ç½®ç»“åˆ
- è¿è¡Œé˜¶æ®µï¼šåœ¨æ‰§è¡Œç¯å¢ƒä¸­å¯åŠ¨åº”ç”¨

æ€ä¹ˆåš (How)ï¼š

å¤šé˜¶æ®µDockerfileï¼š
dockerfile
 æ„å»ºé˜¶æ®µ
FROM python:3.11-slim as builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

 è¿è¡Œé˜¶æ®µ
FROM python:3.11-slim as runner
WORKDIR /app

 ä»æ„å»ºé˜¶æ®µå¤åˆ¶ä¾èµ–
COPY --from=builder /root/.local /root/.local
COPY src/ ./src/

 åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r appuser && useradd -r -g appuser appuser
RUN chown -R appuser:appuser /app
USER appuser

 è¿è¡Œæ—¶é…ç½®
ENV PATH=/root/.local/bin:$PATH
EXPOSE 8000

 å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

CMD ["python", "-m", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]


CI/CD Pipelineç¤ºä¾‹ï¼š
yaml
 .github/workflows/deploy.yml
name: Deploy Application

on:
  push:
    branches: [main]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
       æ„å»ºé˜¶æ®µ
      - name: Build Docker image
        run: |
          docker build -t myapp:${{ github.sha }} .
          docker tag myapp:${{ github.sha }} myapp:latest
      
       å‘å¸ƒé˜¶æ®µ
      - name: Push to registry
        run: |
          docker push myapp:${{ github.sha }}
          docker push myapp:latest
  
  deploy:
    needs: build
    runs-on: ubuntu-latest
    steps:
       è¿è¡Œé˜¶æ®µ
      - name: Deploy to production
        run: |
          kubectl set image deployment/myapp myapp=myapp:${{ github.sha }}
          kubectl rollout status deployment/myapp



 ç¬¬å…­ç« ï¼šè¿›ç¨‹ç®¡ç† (Processes)

 è§„åˆ™ 6.1ï¼šã€å¼ºåˆ¶æ‰§è¡Œã€‘æ— çŠ¶æ€è¿›ç¨‹è®¾è®¡

åšä»€ä¹ˆ (What)ï¼š
- åº”ç”¨è¿›ç¨‹å¿…é¡»æ˜¯æ— çŠ¶æ€å’Œæ— å…±äº«çš„
- æ‰€æœ‰æŒä¹…åŒ–çŠ¶æ€å­˜å‚¨åœ¨åç«¯æœåŠ¡ä¸­
- æ”¯æŒæ°´å¹³æ‰©å±•å’Œè¿›ç¨‹é‡å¯

æ€ä¹ˆåš (How)ï¼š

æ— çŠ¶æ€APIè®¾è®¡ï¼š
python
 main.py
from fastapi import FastAPI, Depends, HTTPException
from services.container import ServiceContainer
from config import settings

app = FastAPI(title=settings.APP_NAME)
container = ServiceContainer(settings)

 ä¾èµ–æ³¨å…¥ï¼Œæ¯ä¸ªè¯·æ±‚è·å–æ–°çš„æœåŠ¡å®ä¾‹
def get_services() -> ServiceContainer:
    return container

@app.get("/chat/{session_id}")
async def get_chat_history(
    session_id: str,
    services: ServiceContainer = Depends(get_services)
):
     ä»å¤–éƒ¨å­˜å‚¨è·å–çŠ¶æ€ï¼Œè€Œéè¿›ç¨‹å†…å­˜
    history = await services.database.execute_query(
        "SELECT  FROM chat_history WHERE session_id = $1",
        {"session_id": session_id}
    )
    return {"history": history}

@app.post("/chat/{session_id}")
async def send_message(
    session_id: str,
    message: dict,
    services: ServiceContainer = Depends(get_services)
):
     å¤„ç†æ¶ˆæ¯ä½†ä¸åœ¨è¿›ç¨‹ä¸­ä¿å­˜çŠ¶æ€
    response = await process_message(message["text"])
    
     çŠ¶æ€æŒä¹…åŒ–åˆ°å¤–éƒ¨å­˜å‚¨
    await services.database.execute_query(
        "INSERT INTO chat_history (session_id, message, response) VALUES ($1, $2, $3)",
        {"session_id": session_id, "message": message["text"], "response": response}
    )
    
    return {"response": response}

 ä¼˜é›…å…³é—­å¤„ç†
import signal
import asyncio

async def graceful_shutdown():
    print("")
     ç­‰å¾…å½“å‰è¯·æ±‚å®Œæˆ
    await asyncio.sleep(1)
     å…³é—­æ•°æ®åº“è¿æ¥æ± 
    await container.database.close()
    await container.cache.close()

def signal_handler(signum, frame):
    asyncio.create_task(graceful_shutdown())

signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)


 ç¬¬ä¸ƒç« ï¼šç«¯å£ç»‘å®š (Port Binding)

 è§„åˆ™ 7.1ï¼šã€å¼ºåˆ¶æ‰§è¡Œã€‘è‡ªåŒ…å«æœåŠ¡

åšä»€ä¹ˆ (What)ï¼š
- åº”ç”¨é€šè¿‡ç«¯å£ç»‘å®šå¯¹å¤–æä¾›æœåŠ¡
- ä¸ä¾èµ–å¤–éƒ¨WebæœåŠ¡å™¨æˆ–åº”ç”¨æœåŠ¡å™¨
- æ”¯æŒæœåŠ¡å‘ç°å’Œå¥åº·æ£€æŸ¥

æ€ä¹ˆåš (How)ï¼š

è‡ªåŒ…å«HTTPæœåŠ¡ï¼š
python
 main.py
import uvicorn
from fastapi import FastAPI
from config import settings

app = FastAPI()

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "version": settings.APP_VERSION,
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/metrics")
async def metrics():
    """PrometheusæŒ‡æ ‡ç«¯ç‚¹"""
    return generate_metrics()

if __name__ == "__main__":
     åº”ç”¨è‡ªå·±ç®¡ç†HTTPæœåŠ¡å™¨
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=int(settings.PORT or 8000),
        workers=int(settings.WORKERS or 1),
        access_log=settings.DEBUG
    )



 ç¬¬å…«ç« ï¼šå¹¶å‘å¤„ç† (Concurrency)

 è§„åˆ™ 8.1ï¼šã€å¼ºåˆ¶æ‰§è¡Œã€‘è¿›ç¨‹æ¨¡å‹æ‰©å±•

åšä»€ä¹ˆ (What)ï¼š
- é€šè¿‡è¿›ç¨‹æ¨¡å‹å®ç°æ°´å¹³æ‰©å±•
- ä¸åŒç±»å‹çš„å·¥ä½œè´Ÿè½½ä½¿ç”¨ä¸åŒçš„è¿›ç¨‹ç±»å‹
- æ”¯æŒåŠ¨æ€æ‰©ç¼©å®¹

æ€ä¹ˆåš (How)ï¼š

å¤šè¿›ç¨‹ç±»å‹è®¾è®¡ï¼š
python
 processes/web.py - HTTPè¯·æ±‚å¤„ç†è¿›ç¨‹
from fastapi import FastAPI
import uvicorn

def run_web_server():
    app = FastAPI()
     ... APIè·¯ç”±å®šä¹‰
    uvicorn.run(app, host="0.0.0.0", port=8000)

 processes/worker.py - åå°ä»»åŠ¡å¤„ç†è¿›ç¨‹
import asyncio
from celery import Celery

celery_app = Celery('agent_worker')

@celery_app.task
async def process_long_running_task(task_data):
     å¤„ç†è€—æ—¶ä»»åŠ¡ï¼Œå¦‚AIæ¨¡å‹æ¨ç†
    result = await ai_model.process(task_data)
    return result

def run_worker():
    celery_app.worker_main(['worker', '--loglevel=info'])

 processes/scheduler.py - å®šæ—¶ä»»åŠ¡è¿›ç¨‹
import schedule
import time

def run_scheduled_tasks():
    schedule.every(1).hours.do(cleanup_old_data)
    schedule.every().day.at("02:00").do(generate_daily_report)
    
    while True:
        schedule.run_pending()
        time.sleep(60)

 Procfile - è¿›ç¨‹é…ç½®æ–‡ä»¶
"""
web: python -m processes.web
worker: python -m processes.worker
scheduler: python -m processes.scheduler
"""


Docker Composeæ‰©å±•é…ç½®ï¼š
yaml
 docker-compose.yml
version: '3.8'
services:
  web:
    build: .
    command: python -m processes.web
    ports:
      - "8000:8000"
    environment:
      - PROCESS_TYPE=web
    deploy:
      replicas: 3
  
  worker:
    build: .
    command: python -m processes.worker
    environment:
      - PROCESS_TYPE=worker
    deploy:
      replicas: 2
  
  scheduler:
    build: .
    command: python -m processes.scheduler
    environment:
      - PROCESS_TYPE=scheduler
    deploy:
      replicas: 1


 ç¬¬ä¹ç« ï¼šæ˜“å¤„ç†æ€§ (Disposability)

 è§„åˆ™ 9.1ï¼šã€å¼ºåˆ¶æ‰§è¡Œã€‘å¿«é€Ÿå¯åŠ¨ä¸ä¼˜é›…å…³é—­

åšä»€ä¹ˆ (What)ï¼š
- åº”ç”¨å¯åŠ¨æ—¶é—´æœ€å°åŒ–
- ä¼˜é›…å¤„ç†SIGTERMä¿¡å·
- æ”¯æŒå¿«é€Ÿé‡å¯å’Œæ•…éšœæ¢å¤

æ€ä¹ˆåš (How)ï¼š

å¿«é€Ÿå¯åŠ¨ä¼˜åŒ–ï¼š
python
 startup.py
import asyncio
import signal
from contextlib import asynccontextmanager
from fastapi import FastAPI

class AppLifecycle:
    def __init__(self):
        self.services = {}
        self.shutdown_event = asyncio.Event()
    
    async def startup(self):
        """å¿«é€Ÿå¯åŠ¨åºåˆ—"""
        print("ğŸš€ Starting application...")
        start_time = time.time()
        
         å¹¶è¡Œåˆå§‹åŒ–æœåŠ¡
        await asyncio.gather(
            self.init_database(),
            self.init_cache(),
            self.init_ai_models(),
        )
        
        startup_time = time.time() - start_time
        print(f"âœ… Application started in {startup_time:.2f}s")
    
    async def init_database(self):
        """æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–"""
        self.services['db'] = await create_db_pool()
    
    async def init_cache(self):
        """ç¼“å­˜è¿æ¥åˆå§‹åŒ–"""
        self.services['cache'] = await create_cache_client()
    
    async def init_ai_models(self):
        """AIæ¨¡å‹é¢„åŠ è½½ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
         åªé¢„åŠ è½½å…³é”®æ¨¡å‹ï¼Œå…¶ä»–æŒ‰éœ€åŠ è½½
        self.services['ai'] = await load_critical_models()
    
    async def shutdown(self):
        """ä¼˜é›…å…³é—­åºåˆ—"""
        print("ğŸ›‘ Shutting down application...")
        
         åœæ­¢æ¥å—æ–°è¯·æ±‚
        self.shutdown_event.set()
        
         ç­‰å¾…ç°æœ‰è¯·æ±‚å®Œæˆï¼ˆæœ€å¤š30ç§’ï¼‰
        await asyncio.sleep(2)
        
         å…³é—­æœåŠ¡è¿æ¥
        await asyncio.gather(
            self.services['db'].close(),
            self.services['cache'].close(),
            return_exceptions=True
        )
        
        print("âœ… Application shutdown complete")

 å…¨å±€ç”Ÿå‘½å‘¨æœŸç®¡ç†
lifecycle = AppLifecycle()

@asynccontextmanager
async def lifespan(app: FastAPI):
     å¯åŠ¨
    await lifecycle.startup()
    yield
     å…³é—­
    await lifecycle.shutdown()

app = FastAPI(lifespan=lifespan)

 ä¿¡å·å¤„ç†
def setup_signal_handlers():
    def signal_handler(signum, frame):
        print(f"Received signal {signum}")
        asyncio.create_task(lifecycle.shutdown())
    
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)

setup_signal_handlers()



 ç¬¬åç« ï¼šå¼€å‘ç”Ÿäº§ä¸€è‡´æ€§ (Dev/Prod Parity)

 è§„åˆ™ 10.1ï¼šã€å¼ºåˆ¶æ‰§è¡Œã€‘ç¯å¢ƒä¸€è‡´æ€§

åšä»€ä¹ˆ (What)ï¼š
- å¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ç¯å¢ƒå°½å¯èƒ½ä¸€è‡´
- ä½¿ç”¨ç›¸åŒçš„åç«¯æœåŠ¡ç‰ˆæœ¬
- æœ€å°åŒ–ç¯å¢ƒé—´çš„å·®å¼‚

æ€ä¹ˆåš (How)ï¼š

Dockerå¼€å‘ç¯å¢ƒï¼š
yaml
 docker-compose.dev.yml
version: '3.8'
services:
  app:
    build: 
      context: .
      target: runner
    volumes:
      - ./src:/app/src   å¼€å‘æ—¶ä»£ç çƒ­é‡è½½
    environment:
      - DEBUG=true
      - DATABASE_URL=postgresql://dev:dev@postgres:5432/agent_dev
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - redis

  postgres:
    image: postgres:15   ä¸ç”Ÿäº§ç¯å¢ƒç›¸åŒç‰ˆæœ¬
    environment:
      POSTGRES_DB: agent_dev
      POSTGRES_USER: dev
      POSTGRES_PASSWORD: dev
    volumes:
      - postgres_dev:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine   ä¸ç”Ÿäº§ç¯å¢ƒç›¸åŒç‰ˆæœ¬
    ports:
      - "6379:6379"

volumes:
  postgres_dev:


ç¯å¢ƒé…ç½®ç®¡ç†ï¼š
python
 config/environments.py
from enum import Enum
from pydantic_settings import BaseSettings

class Environment(str, Enum):
    DEVELOPMENT = "development"
    TESTING = "testing"
    STAGING = "staging"
    PRODUCTION = "production"

class BaseConfig(BaseSettings):
    ENVIRONMENT: Environment = Environment.DEVELOPMENT
    
     æ•°æ®åº“é…ç½® - æ‰€æœ‰ç¯å¢ƒä½¿ç”¨ç›¸åŒçš„PostgreSQL
    DATABASE_URL: str
    DATABASE_POOL_SIZE: int = 5
    DATABASE_POOL_MAX_OVERFLOW: int = 10
    
     Redisé…ç½® - æ‰€æœ‰ç¯å¢ƒä½¿ç”¨ç›¸åŒç‰ˆæœ¬
    REDIS_URL: str
    
    class Config:
        case_sensitive = True

class DevelopmentConfig(BaseConfig):
    DEBUG: bool = True
    LOG_LEVEL: str = "DEBUG"
    
     å¼€å‘ç¯å¢ƒå¯ä»¥ä½¿ç”¨æ›´å®½æ¾çš„è®¾ç½®
    DATABASE_POOL_SIZE: int = 2

class ProductionConfig(BaseConfig):
    DEBUG: bool = False
    LOG_LEVEL: str = "INFO"
    
     ç”Ÿäº§ç¯å¢ƒéœ€è¦æ›´ä¸¥æ ¼çš„è®¾ç½®
    DATABASE_POOL_SIZE: int = 20
    DATABASE_POOL_MAX_OVERFLOW: int = 30

def get_config() -> BaseConfig:
    env = Environment(os.getenv("ENVIRONMENT", "development"))
    
    config_map = {
        Environment.DEVELOPMENT: DevelopmentConfig,
        Environment.PRODUCTION: ProductionConfig,
    }
    
    return config_map[env]()


 ç¬¬åä¸€ç« ï¼šæ—¥å¿—ç®¡ç† (Logs)

 è§„åˆ™ 11.1ï¼šã€å¼ºåˆ¶æ‰§è¡Œã€‘ç»“æ„åŒ–æ—¥å¿—æµ

åšä»€ä¹ˆ (What)ï¼š
- å°†æ—¥å¿—è§†ä¸ºäº‹ä»¶æµè¾“å‡ºåˆ°stdout
- ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—æ ¼å¼ï¼ˆJSONï¼‰
- æ”¯æŒåˆ†å¸ƒå¼è¿½è¸ªå’Œç›‘æ§

æ€ä¹ˆåš (How)ï¼š

ç»“æ„åŒ–æ—¥å¿—é…ç½®ï¼š
python
 logging_config.py
import logging
import json
import sys
from datetime import datetime
from typing import Any, Dict
from pythonjsonlogger import jsonlogger

class CustomJsonFormatter(jsonlogger.JsonFormatter):
    def add_fields(self, log_record: Dict[str, Any], record: logging.LogRecord, message_dict: Dict[str, Any]):
        super().add_fields(log_record, record, message_dict)
        
         æ·»åŠ æ ‡å‡†å­—æ®µ
        log_record['timestamp'] = datetime.utcnow().isoformat()
        log_record['level'] = record.levelname
        log_record['service'] = 'ai-agent'
        log_record['version'] = os.getenv('APP_VERSION', '1.0.0')
        
         æ·»åŠ è¿½è¸ªä¿¡æ¯
        if hasattr(record, 'trace_id'):
            log_record['trace_id'] = record.trace_id
        if hasattr(record, 'span_id'):
            log_record['span_id'] = record.span_id

def setup_logging():
     æ ¹æ—¥å¿—å™¨é…ç½®
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
     æ¸…é™¤é»˜è®¤å¤„ç†å™¨
    root_logger.handlers.clear()
    
     åˆ›å»ºstdoutå¤„ç†å™¨
    handler = logging.StreamHandler(sys.stdout)
    formatter = CustomJsonFormatter(
        '%(timestamp)s %(level)s %(service)s %(message)s'
    )
    handler.setFormatter(formatter)
    root_logger.addHandler(handler)

 åº”ç”¨æ—¥å¿—å™¨
logger = logging.getLogger(__name__)

 ä½¿ç”¨ç¤ºä¾‹
@app.middleware("http")
async def logging_middleware(request: Request, call_next):
    trace_id = request.headers.get('x-trace-id', str(uuid.uuid4()))
    
     æ·»åŠ è¿½è¸ªä¸Šä¸‹æ–‡
    extra = {
        'trace_id': trace_id,
        'method': request.method,
        'path': request.url.path,
        'user_agent': request.headers.get('user-agent')
    }
    
    start_time = time.time()
    logger.info("Request started", extra=extra)
    
    try:
        response = await call_next(request)
        
         è®°å½•æˆåŠŸå“åº”
        duration = time.time() - start_time
        extra.update({
            'status_code': response.status_code,
            'duration_ms': round(duration  1000, 2)
        })
        logger.info("Request completed", extra=extra)
        
        return response
        
    except Exception as e:
         è®°å½•é”™è¯¯
        duration = time.time() - start_time
        extra.update({
            'error': str(e),
            'error_type': type(e).__name__,
            'duration_ms': round(duration  1000, 2)
        })
        logger.error("Request failed", extra=extra)
        raise


æ—¥å¿—èšåˆé…ç½®ï¼ˆELK Stackï¼‰ï¼š
yaml
 docker-compose.logging.yml
version: '3.8'
services:
  app:
    build: .
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
  
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    volumes:
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
    depends_on:
      - elasticsearch

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch


 ç¬¬åäºŒç« ï¼šç®¡ç†è¿›ç¨‹ (Admin Processes)

 è§„åˆ™ 12.1ï¼šã€å¼ºåˆ¶æ‰§è¡Œã€‘ä¸€æ¬¡æ€§ç®¡ç†ä»»åŠ¡

åšä»€ä¹ˆ (What)ï¼š
- ç®¡ç†ä»»åŠ¡ä½œä¸ºç‹¬ç«‹çš„ä¸€æ¬¡æ€§è¿›ç¨‹è¿è¡Œ
- ä½¿ç”¨ä¸åº”ç”¨ç›¸åŒçš„ç¯å¢ƒå’Œä»£ç åº“
- æ”¯æŒæ•°æ®åº“è¿ç§»ã€æ•°æ®å¯¼å…¥ç­‰ç®¡ç†æ“ä½œ

æ€ä¹ˆåš (How)ï¼š

ç®¡ç†ä»»åŠ¡æ¡†æ¶ï¼š
python
 management/base.py
import asyncio
import sys
from abc import ABC, abstractmethod
from typing import List
from config import settings
from services.container import ServiceContainer

class BaseCommand(ABC):
    """ç®¡ç†å‘½ä»¤åŸºç±»"""
    
    def __init__(self):
        self.services = ServiceContainer(settings)
    
    @property
    @abstractmethod
    def name(self) -> str:
        """å‘½ä»¤åç§°"""
        pass
    
    @property
    @abstractmethod
    def description(self) -> str:
        """å‘½ä»¤æè¿°"""
        pass
    
    @abstractmethod
    async def handle(self, args, kwargs):
        """å‘½ä»¤æ‰§è¡Œé€»è¾‘"""
        pass
    
    async def run(self, args: List[str]):
        """è¿è¡Œå‘½ä»¤"""
        try:
            print(f"Running command: {self.name}")
            await self.handle(args)
            print(f"Command completed: {self.name}")
        except Exception as e:
            print(f"Command failed: {self.name} - {e}")
            sys.exit(1)
        finally:
             æ¸…ç†èµ„æº
            await self.cleanup()
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self.services, 'database'):
            await self.services.database.close()
        if hasattr(self.services, 'cache'):
            await self.services.cache.close()

 management/commands/migrate.py
from management.base import BaseCommand

class MigrateCommand(BaseCommand):
    name = "migrate"
    description = "Run database migrations"
    
    async def handle(self, args):
        migrations = await self.get_pending_migrations()
        
        for migration in migrations:
            print(f"Applying migration: {migration.name}")
            await migration.apply(self.services.database)
            await self.mark_migration_applied(migration)
    
    async def get_pending_migrations(self):
         è·å–å¾…æ‰§è¡Œçš„è¿ç§»
        pass
    
    async def mark_migration_applied(self, migration):
         æ ‡è®°è¿ç§»å·²æ‰§è¡Œ
        pass

 management/commands/seed_data.py
class SeedDataCommand(BaseCommand):
    name = "seed"
    description = "Seed initial data"
    
    async def handle(self, args):
         æ’å…¥åˆå§‹æ•°æ®
        await self.create_default_users()
        await self.create_sample_conversations()
    
    async def create_default_users(self):
         åˆ›å»ºé»˜è®¤ç”¨æˆ·
        pass

 management/cli.py
import sys
import asyncio
from management.commands.migrate import MigrateCommand
from management.commands.seed_data import SeedDataCommand

COMMANDS = {
    'migrate': MigrateCommand,
    'seed': SeedDataCommand,
}

async def main():
    if len(sys.argv) < 2:
        print("Available commands:")
        for name, cmd_class in COMMANDS.items():
            print(f"  {name}: {cmd_class().description}")
        sys.exit(1)
    
    command_name = sys.argv[1]
    command_args = sys.argv[2:]
    
    if command_name not in COMMANDS:
        print(f"Unknown command: {command_name}")
        sys.exit(1)
    
    command = COMMANDS[command_name]()
    await command.run(command_args)

if __name__ == "__main__":
    asyncio.run(main())


Kubernetes Jobé…ç½®ï¼š
yaml
 k8s-migrate-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: ai-agent-migrate
spec:
  template:
    spec:
      containers:
      - name: migrate
        image: myapp:latest
        command: ["python", "-m", "management.cli", "migrate"]
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
      restartPolicy: Never
  backoffLimit: 3


 ç¬¬åä¸‰ç« ï¼šå®‰å…¨ä¸ç›‘æ§ (Security & Monitoring)

 è§„åˆ™ 13.1ï¼šã€å¼ºåˆ¶æ‰§è¡Œã€‘å…¨é¢å®‰å…¨é˜²æŠ¤

åšä»€ä¹ˆ (What)ï¼š
- å®æ–½å¤šå±‚å®‰å…¨é˜²æŠ¤
- é›†æˆç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ
- æ”¯æŒå®‰å…¨å®¡è®¡å’Œåˆè§„æ€§

æ€ä¹ˆåš (How)ï¼š

å®‰å…¨ä¸­é—´ä»¶ï¼š
python
 security/middleware.py
import hashlib
import hmac
import time
from fastapi import Request, HTTPException
from starlette.middleware.base import BaseHTTPMiddleware

class SecurityMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
         1. é€Ÿç‡é™åˆ¶
        await self.rate_limit_check(request)
        
         2. è¯·æ±‚éªŒè¯
        await self.validate_request(request)
        
         3. å®‰å…¨å¤´è®¾ç½®
        response = await call_next(request)
        self.set_security_headers(response)
        
        return response
    
    async def rate_limit_check(self, request: Request):
        client_ip = request.client.host
        cache_key = f"rate_limit:{client_ip}"
        
        current_requests = await self.cache.get(cache_key) or 0
        if int(current_requests) > 100:   æ¯åˆ†é’Ÿ100æ¬¡è¯·æ±‚
            raise HTTPException(429, "Rate limit exceeded")
        
        await self.cache.set(cache_key, int(current_requests) + 1, ttl=60)
    
    def set_security_headers(self, response):
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "DENY"
        response.headers["X-XSS-Protection"] = "1; mode=block"
        response.headers["Strict-Transport-Security"] = "max-age=31536000"

 monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge
import time

 å®šä¹‰æŒ‡æ ‡
REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('http_request_duration_seconds', 'HTTP request duration')
ACTIVE_CONNECTIONS = Gauge('active_connections', 'Active connections')

class MetricsMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()        
        try:
            response = await call_next(request)            
             è®°å½•æˆåŠŸæŒ‡æ ‡
            REQUEST_COUNT.labels(
                method=request.method,
                endpoint=request.url.path,
                status=response.status_code
            ).inc()            
            return response            
        except Exception as e:
             è®°å½•é”™è¯¯æŒ‡æ ‡
            REQUEST_COUNT.labels(
                method=request.method,
                endpoint=request.url.path,
                status=500
            ).inc()
            raise            
        finally:
             è®°å½•è¯·æ±‚æ—¶é•¿
            REQUEST_DURATION.observe(time.time() - start_time)


ç›‘æ§é…ç½®ï¼š
yaml
 monitoring/prometheus.yml
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: 'ai-agent'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    scrape_interval: 5s

 monitoring/grafana-dashboard.json
{
  "dashboard": {
    "title": "AI Agent Monitoring",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[1m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph", 
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[1m])",
            "legendFormat": "5xx Errors"
          }
        ]
      }
    ]
  }
}

 ç¬¬åå››ç« ï¼šæµ‹è¯•ç­–ç•¥ (Testing Strategy)

 è§„åˆ™ 14.1ï¼šã€å¼ºåˆ¶æ‰§è¡Œã€‘å¤šå±‚æ¬¡æµ‹è¯•

åšä»€ä¹ˆ (What)ï¼š
- å®æ–½å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€ç«¯åˆ°ç«¯æµ‹è¯•
- æµ‹è¯•è¦†ç›–ç‡è¦æ±‚è¾¾åˆ°80%ä»¥ä¸Š
- æ”¯æŒæµ‹è¯•é©±åŠ¨å¼€å‘ï¼ˆTDDï¼‰

æ€ä¹ˆåš (How)ï¼š

æµ‹è¯•æ¡†æ¶é…ç½®ï¼š
python
 tests/conftest.py
import pytest
import asyncio
from httpx import AsyncClient
from main import app
from services.container import ServiceContainer
from config import TestConfig

@pytest.fixture(scope="session")
def event_loop():
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
async def test_client():
    async with AsyncClient(app=app, base_url="http://test") as client:
        yield client

@pytest.fixture
async def test_services():
    config = TestConfig()
    services = ServiceContainer(config)
    yield services
     æ¸…ç†
    await services.cleanup()

 tests/unit/test_services.py
import pytest
from services.llm_service import LLMService
from unittest.mock import AsyncMock, patch

class TestLLMService:
    @pytest.fixture
    def llm_service(self, test_services):
        return LLMService(test_services.config)
    
    @patch('openai.ChatCompletion.acreate')
    async def test_generate_response(self, mock_openai, llm_service):
         æ¨¡æ‹ŸOpenAIå“åº”
        mock_openai.return_value = {
            'choices': [{'message': {'content': 'Test response'}}]
        }
        
        response = await llm_service.generate_response("Test prompt")
        
        assert response == "Test response"
        mock_openai.assert_called_once()

 tests/integration/test_api.py
import pytest

class TestChatAPI:
    async def test_create_chat_session(self, test_client):
        response = await test_client.post("/chat/sessions")
        
        assert response.status_code == 201
        data = response.json()
        assert "session_id" in data
    
    async def test_send_message(self, test_client):
         åˆ›å»ºä¼šè¯
        session_response = await test_client.post("/chat/sessions")
        session_id = session_response.json()["session_id"]
        
         å‘é€æ¶ˆæ¯
        message_response = await test_client.post(
            f"/chat/sessions/{session_id}/messages",
            json={"text": "Hello"}
        )
        
        assert message_response.status_code == 200
        data = message_response.json()
        assert "response" in data

 tests/e2e/test_workflow.py
import pytest

class TestCompleteWorkflow:
    async def test_full_conversation_flow(self, test_client):
        """æµ‹è¯•å®Œæ•´å¯¹è¯æµç¨‹"""
         1. åˆ›å»ºç”¨æˆ·ä¼šè¯
        session_resp = await test_client.post("/chat/sessions")
        session_id = session_resp.json()["session_id"]
        
         2. å‘é€å¤šè½®å¯¹è¯
        messages = ["H", "W?", "T"]  
        for message in messages:
            resp = await test_client.post(
                f"/chat/sessions/{session_id}/messages",
                json={"text": message}
            )
            assert resp.status_code == 200  
         3. è·å–å¯¹è¯å†å²
        history_resp = await test_client.get(f"/chat/sessions/{session_id}/history")
        history = history_resp.json()["history"]     
        assert len(history) == len(messages)


      